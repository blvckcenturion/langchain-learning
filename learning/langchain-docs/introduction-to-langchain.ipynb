{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b7fdaf",
   "metadata": {},
   "source": [
    "# Introduccion a LangChain\n",
    "\n",
    "<hr>\n",
    "\n",
    "## ¿Que es LangChain?\n",
    "\n",
    "LangChain es un framework emergente que permite a sus usuarios crear rapidamente aplicaciones y canales al rededor de \n",
    "modelos de lenguaje.\n",
    "\n",
    "Puede ser utilizado para la creacion de chatbots, respuestas generativas de preguntas, resumenes y mucho mas.\n",
    "\n",
    "La idea central de la biblioteca es que podemos \"encadenar\" distintos componentes para crear casos de uso mas avanzados entorno a modelos de lenguaje.\n",
    "\n",
    "LangChain proporciona muchos módulos que se pueden usar para crear aplicaciones de modelos de lenguaje. Los módulos se pueden combinar para crear aplicaciones más complejas o se pueden usar individualmente para aplicaciones simples.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Instalacion\n",
    "Para empezar debe instalar LangChain con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6c676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (0.0.133)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (1.24.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fd2ca",
   "metadata": {},
   "source": [
    "## Configuracion del entorno\n",
    "\n",
    "Utilizar LangChain generalmente requerira integraciones con uno o mas proveedores de modelos, almacenes de datos, APIs, etc.\n",
    "\n",
    "Para este ejemplo utilizaremos las APIs de OpenAI, para lo cual debemos instalar su SDK de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0325a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (0.27.4)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834635d2",
   "metadata": {},
   "source": [
    "Despues necesitamos proveer una clave de API de OpenAI para poder hacer uso del mismo.\n",
    "\n",
    "Para conseguir una clave de API dirigase a: [OpenAI](https://openai.com/product)\n",
    "\n",
    "Una vez contamos con una clave, procedemos a declararla como una variable de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65ee762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f114f9",
   "metadata": {},
   "source": [
    "## Utilizando LangChain para la construccion de modelos de lenguaje\n",
    "\n",
    "LangChain proporciona varios modulos que se pueden utilizar para la creacion de aplicaciones de modelos de lenguaje. Los modulos se pueden combinar para crear aplicaciones mas complejas o se pueden usar individualmente para aplicaciones simples, entre ellos podemos destacar:\n",
    "\n",
    "### Prompt Templates\n",
    "En informática, un \"prompt\" o indicacion se refiere a una solicitud o mensaje que aparece en la pantalla de un dispositivo, solicitando al usuario que realice una acción o proporcione información.\n",
    "\n",
    "Por ejemplo, un \"prompt\" de inicio de sesión solicita al usuario que proporcione su nombre de usuario y contraseña para acceder a una cuenta.\n",
    "\n",
    "La nueva forma de programar modelos de lenguaje es a través de prompts. Un prompt se refiere a la entrada al modelo. Esta entrada rara vez está codificada de forma fija, sino que a menudo se construye a partir de múltiples componentes.\n",
    "\n",
    "Un PromptTemplate es responsable de la construcción de esta entrada. LangChain proporciona varias clases y funciones para facilitar la construcción y el trabajo con \"prompts\".\n",
    "\n",
    "### Indices\n",
    "\n",
    "Los índices (Indexes) se refieren a formas de estructurar documentos para que los LLM puedan interactuar mejor con ellos.\n",
    "\n",
    "### Cadenas\n",
    "\n",
    "Utilizar LLMs de forma aislada es adecuado para ciertas aplicaciones simples, pero existen casos mas complejos donde se requiere encadenar LLMs, ya sea entre si o con otras herramientas, la union de estas mismas es denominada cadena (chain).\n",
    "\n",
    "\n",
    "### Modelos\n",
    "\n",
    "#### LLMs\n",
    "\n",
    "Un Gran Modelo de Lenguaje (Large Lenguage Model, por sus siglas en inglés) es un tipo de algoritmo de IA que se entrena con grandes cantidades de datos de lenguaje natural para aprender a generar lenguaje coherente y similar al humano, los cuales utilizan técnicas avanzadas de aprendizaje automático(ML), como redes neuronales profundas, para analizar y comprender la estructura y los patrones del lenguaje, y para generar nuevo lenguaje en función de ese conocimiento.\n",
    "\n",
    "Los LLM son capaces de realizar una amplia gama de tareas de procesamiento de lenguaje natural (NLP), incluyendo traducción de idiomas, análisis de sentimientos, resumen de texto, completado de texto y respuestas a preguntas, entre otros.\n",
    "\n",
    "Uno de los ejemplos más conocidos de un LLM es la serie GPT (Generative Pre-trained Transformer) de OpenAI, pero asi como este existen otros ejemplos como ser:\n",
    "\n",
    "- [BLOOM | BigScience](https://bigscience.huggingface.co/blog/bloom)\n",
    "- [LaMDA | Google](https://blog.google/technology/ai/lamda/)\n",
    "- [MT-NLG | Nvidia y Microsoft](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n",
    "- [LLaMA | Meta AI](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n",
    "\n",
    "\n",
    "LangChain es compatible con varios proveedores de LLM, como Hugging Face y OpenAI.\n",
    "\n",
    "Estos modelos toman una cadena de texto como entrada y devuelven una cadena de texto como salida.\n",
    "\n",
    "#### Modelos de Chat\n",
    "\n",
    "Un modelo de chat, también conocido como modelo conversacional o chatbot, es un modelo de IA que simula una conversación humana a través de interacciones basadas en texto o voz. Estos modelos utilizan técnicas de procesamiento de lenguaje natural (PLN) para entender las entradas de los usuarios y generar respuestas que sean contextualmente relevantes y coherentes.\n",
    "\n",
    "Estos modelos suelen estar respaldados por un modelo de lenguaje, pero sus APIs están más estructuradas. Específicamente, estos modelos toman una lista de mensajes de chat como entrada y devuelven un mensaje de chat.\n",
    "\n",
    "Entre los modelos de chat mas destacados podemos encontrar:\n",
    "\n",
    "- [ChatGPT | OpenAI](https://openai.com/blog/chatgpt)\n",
    "- [Bing Chat | Microsoft](https://www.bing.com/new)\n",
    "- [Bard | Google](https://blog.google/technology/ai/bard-google-ai-search-updates/)\n",
    "\n",
    "\n",
    "#### Modelos de Embedding de Texto\n",
    "\n",
    "Un modelo de embedding (incrustacion por su traduccion al español) de texto es un modelo de IA utilizado para representar palabras o frases en una forma numérica que puede ser fácilmente procesada por algoritmos de aprendizaje automático. Este modelo asigna a cada palabra o frase un vector en un espacio de alta dimensión donde las palabras con significados similares están ubicadas cerca unas de otras.\n",
    "\n",
    "Los modelos de embedding de texto se crean típicamente utilizando arquitecturas de redes neuronales como:\n",
    "\n",
    "- [Word2Vec](https://www.coveo.com/blog/word2vec-explained/)\n",
    "- [GloVe](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b)\n",
    "\n",
    "Estos modelos pueden ser entrenados con grandes cantidades de datos de texto para aprender las relaciones entre palabras y para identificar patrones y correlaciones en el texto.\n",
    "\n",
    "Una vez entrenado, el modelo de embedding de texto puede ser utilizado para una variedad de tareas de procesamiento de lenguaje natural como análisis de sentimiento, clasificación de texto y traducción automática. Por ejemplo, en el análisis de sentimiento, el modelo tomaría un texto y lo convertiría en un vector, que luego se puede alimentar a un clasificador para predecir si el sentimiento del texto es positivo o negativo.\n",
    "\n",
    "### Agentes\n",
    "\n",
    "Los agentes usan LLM para decidir qué acciones se deben tomar, se pueden usar herramientas como búsqueda web o calculadoras, y todo se empaqueta en un ciclo lógico de operaciones.\n",
    "\n",
    "\n",
    "### Memoria\n",
    "\n",
    "Por defecto, las cadenas y los agentes no tienen estado, lo que significa que tratan cada consulta de forma independiente, en algunas aplicaciones (los chatbots son un GRAN ejemplo) es muy importante recordar las interacciones anteriores, tanto a corto como a largo plazo, el concepto de Memoria en LangChain existe precisamente para hacer eso.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Construyendo una Aplicacion de Modelo de Lenguaje: LLMs\n",
    "\n",
    "Ahora que hemos instalado LangChain y configurado nuestro entorno, podemos comenzar a construir nuestra aplicación de modelo de lenguaje.\n",
    "\n",
    "### LLMs: Obtener predicciones de un modelo de lenguaje\n",
    "\n",
    "El bloque de construcción más básico de LangChain es llamar a un LLM en alguna entrada.\n",
    "\n",
    "Para este ejemplo, pretendamos que estamos construyendo un servicio que genera nombres para ideas de negocio basado en lo que hace el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087421bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calcetines Rainbow Joy.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Inicializamos el LLM\n",
    "# En este ejemplo, probablemente queramos que las salidas sean MÁS aleatorias.\n",
    "# para esto utilizamos el argumento temperature.\n",
    "# nos permite controlar que tan deterministicas son las respuestas.\n",
    "# donde 0 significa que son deterministicas y 1 totalmente aleatorias.\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# Indicamos nuestra entrada\n",
    "text = \"Cual seria un buen nombre para una empresa que vende calcetines coloridos?\"\n",
    "\n",
    "# Llamamos al modelo pasando como argumento nuestra entrada\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f17b59",
   "metadata": {},
   "source": [
    "### Prompt Templates: Administrar prompts para LLMs\n",
    "\n",
    "Normalmente cuando utilizamos un LLM en una aplicacion, no enviamos la entrada del usuario directamente an LLM, sino que queremos tomar la entrada del usuario, construir un prompt adecuado y enviar eso al LLM.\n",
    "\n",
    "En el ejemplo anterior, el texto de entrada estaba predeterminado para pedir el nombre de una empresa que fabricaba calcetines coloridos, lo que quisieramos hacer es solamente tomar la entrada del usuario sobre que hace la empresa y crear un prompt con esta informacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541ac59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cual seria un buen nombre para una empresa que vende panqueques esponjosos\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Instanciamos un PromptTemplate donde indicamos nuestra entrada predeterminada y las variables a cambiar\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Cual seria un buen nombre para una empresa que vende {product}\"\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"panqueques esponjosos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7807b5",
   "metadata": {},
   "source": [
    "## Cadenas: Combinando LLMs y prompts \n",
    "\n",
    "Una cadena en LangChain esta compuesta por multiples componentes, que pueden ser primitivos como LLMs o otras cadenas.\n",
    "\n",
    "El tipo de cadena mas basico es un LLMChain que consta de un PromptTemplate y un LLM.\n",
    "\n",
    "Extendiendo el ejemplo anterior, podemos construir un LLMChain que toma la entrada del usuario, lo formatea con un PromptTemplate y luego pasa la respuesta formateada a un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "798e3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Fluffy Pancake Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Ahora podemos correr nuestra cadena solamente especificando el producto deseado.\n",
    "print(chain.run(\"Panqueques esponjosos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5bb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
