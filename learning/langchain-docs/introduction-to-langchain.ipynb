{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b7fdaf",
   "metadata": {},
   "source": [
    "# Introduccion a LangChain\n",
    "\n",
    "<hr>\n",
    "\n",
    "## ¿Que es LangChain?\n",
    "\n",
    "LangChain es un framework emergente que permite a sus usuarios crear rapidamente aplicaciones y canales al rededor de \n",
    "modelos de lenguaje.\n",
    "\n",
    "Puede ser utilizado para la creacion de chatbots, respuestas generativas de preguntas, resumenes y mucho mas.\n",
    "\n",
    "La idea central de la biblioteca es que podemos \"encadenar\" distintos componentes para crear casos de uso mas avanzados entorno a modelos de lenguaje.\n",
    "\n",
    "LangChain proporciona muchos módulos que se pueden usar para crear aplicaciones de modelos de lenguaje. Los módulos se pueden combinar para crear aplicaciones más complejas o se pueden usar individualmente para aplicaciones simples.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Instalacion\n",
    "Para empezar debe instalar LangChain con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6c676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.134-py3-none-any.whl (510 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.7/510.7 kB\u001b[0m \u001b[31m954.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m915.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML>=5.4.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting SQLAlchemy<2,>=1\n",
      "  Downloading SQLAlchemy-1.4.47-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2,>=1\n",
      "  Downloading pydantic-1.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/blvck/.local/lib/python3.8/site-packages (from langchain) (2.28.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /home/blvck/.local/lib/python3.8/site-packages (from langchain) (1.24.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.0.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/blvck/.local/lib/python3.8/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from SQLAlchemy<2,>=1->langchain) (0.4.15)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/blvck/.local/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, SQLAlchemy, PyYAML, pydantic, mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
      "Successfully installed PyYAML-6.0 SQLAlchemy-1.4.47 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.134 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 pydantic-1.10.7 tenacity-8.2.2 typing-inspect-0.8.0 yarl-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fd2ca",
   "metadata": {},
   "source": [
    "## Configuracion del entorno\n",
    "\n",
    "Utilizar LangChain generalmente requerira integraciones con uno o mas proveedores de modelos, almacenes de datos, APIs, etc.\n",
    "\n",
    "Para este ejemplo utilizaremos las APIs de OpenAI, para lo cual debemos instalar su SDK de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0325a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m275.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/blvck/.local/lib/python3.8/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in /home/blvck/.local/lib/python3.8/site-packages (from openai) (2.28.2)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m169.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/blvck/.local/lib/python3.8/site-packages (from requests>=2.20->openai) (3.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/blvck/.local/lib/python3.8/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Installing collected packages: tqdm, openai\n",
      "Successfully installed openai-0.27.4 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834635d2",
   "metadata": {},
   "source": [
    "Despues necesitamos proveer una clave de API de OpenAI para poder hacer uso del mismo.\n",
    "\n",
    "Para conseguir una clave de API dirigase a: [OpenAI](https://openai.com/product)\n",
    "\n",
    "Una vez contamos con una clave, procedemos a declararla como una variable de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65ee762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f114f9",
   "metadata": {},
   "source": [
    "## Utilizando LangChain para la construccion de modelos de lenguaje\n",
    "\n",
    "LangChain proporciona varios modulos que se pueden utilizar para la creacion de aplicaciones de modelos de lenguaje. Los modulos se pueden combinar para crear aplicaciones mas complejas o se pueden usar individualmente para aplicaciones simples, entre ellos podemos destacar:\n",
    "\n",
    "### Prompt Templates\n",
    "En informática, un \"prompt\" o indicacion se refiere a una solicitud o mensaje que aparece en la pantalla de un dispositivo, solicitando al usuario que realice una acción o proporcione información.\n",
    "\n",
    "Por ejemplo, un \"prompt\" de inicio de sesión solicita al usuario que proporcione su nombre de usuario y contraseña para acceder a una cuenta.\n",
    "\n",
    "La nueva forma de programar modelos de lenguaje es a través de prompts. Un prompt se refiere a la entrada al modelo. Esta entrada rara vez está codificada de forma fija, sino que a menudo se construye a partir de múltiples componentes.\n",
    "\n",
    "Un PromptTemplate es responsable de la construcción de esta entrada. LangChain proporciona varias clases y funciones para facilitar la construcción y el trabajo con \"prompts\".\n",
    "\n",
    "### Indices\n",
    "\n",
    "Los índices (Indexes) se refieren a formas de estructurar documentos para que los LLM puedan interactuar mejor con ellos.\n",
    "\n",
    "### Cadenas\n",
    "\n",
    "Utilizar LLMs de forma aislada es adecuado para ciertas aplicaciones simples, pero existen casos mas complejos donde se requiere encadenar LLMs, ya sea entre si o con otras herramientas, la union de estas mismas es denominada cadena (chain).\n",
    "\n",
    "\n",
    "### Modelos\n",
    "\n",
    "#### LLMs\n",
    "\n",
    "Un Gran Modelo de Lenguaje (Large Lenguage Model, por sus siglas en inglés) es un tipo de algoritmo de IA que se entrena con grandes cantidades de datos de lenguaje natural para aprender a generar lenguaje coherente y similar al humano, los cuales utilizan técnicas avanzadas de aprendizaje automático(ML), como redes neuronales profundas, para analizar y comprender la estructura y los patrones del lenguaje, y para generar nuevo lenguaje en función de ese conocimiento.\n",
    "\n",
    "Los LLM son capaces de realizar una amplia gama de tareas de procesamiento de lenguaje natural (NLP), incluyendo traducción de idiomas, análisis de sentimientos, resumen de texto, completado de texto y respuestas a preguntas, entre otros.\n",
    "\n",
    "Uno de los ejemplos más conocidos de un LLM es la serie GPT (Generative Pre-trained Transformer) de OpenAI, pero asi como este existen otros ejemplos como ser:\n",
    "\n",
    "- [BLOOM | BigScience](https://bigscience.huggingface.co/blog/bloom)\n",
    "- [LaMDA | Google](https://blog.google/technology/ai/lamda/)\n",
    "- [MT-NLG | Nvidia y Microsoft](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n",
    "- [LLaMA | Meta AI](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n",
    "\n",
    "\n",
    "LangChain es compatible con varios proveedores de LLM, como Hugging Face y OpenAI.\n",
    "\n",
    "Estos modelos toman una cadena de texto como entrada y devuelven una cadena de texto como salida.\n",
    "\n",
    "#### Modelos de Chat\n",
    "\n",
    "Un modelo de chat, también conocido como modelo conversacional o chatbot, es un modelo de IA que simula una conversación humana a través de interacciones basadas en texto o voz. Estos modelos utilizan técnicas de procesamiento de lenguaje natural (PLN) para entender las entradas de los usuarios y generar respuestas que sean contextualmente relevantes y coherentes.\n",
    "\n",
    "Estos modelos suelen estar respaldados por un modelo de lenguaje, pero sus APIs están más estructuradas. Específicamente, estos modelos toman una lista de mensajes de chat como entrada y devuelven un mensaje de chat.\n",
    "\n",
    "Entre los modelos de chat mas destacados podemos encontrar:\n",
    "\n",
    "- [ChatGPT | OpenAI](https://openai.com/blog/chatgpt)\n",
    "- [Bing Chat | Microsoft](https://www.bing.com/new)\n",
    "- [Bard | Google](https://blog.google/technology/ai/bard-google-ai-search-updates/)\n",
    "\n",
    "\n",
    "#### Modelos de Embedding de Texto\n",
    "\n",
    "Un modelo de embedding (incrustacion por su traduccion al español) de texto es un modelo de IA utilizado para representar palabras o frases en una forma numérica que puede ser fácilmente procesada por algoritmos de aprendizaje automático. Este modelo asigna a cada palabra o frase un vector en un espacio de alta dimensión donde las palabras con significados similares están ubicadas cerca unas de otras.\n",
    "\n",
    "Los modelos de embedding de texto se crean típicamente utilizando arquitecturas de redes neuronales como:\n",
    "\n",
    "- [Word2Vec](https://www.coveo.com/blog/word2vec-explained/)\n",
    "- [GloVe](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b)\n",
    "\n",
    "Estos modelos pueden ser entrenados con grandes cantidades de datos de texto para aprender las relaciones entre palabras y para identificar patrones y correlaciones en el texto.\n",
    "\n",
    "Una vez entrenado, el modelo de embedding de texto puede ser utilizado para una variedad de tareas de procesamiento de lenguaje natural como análisis de sentimiento, clasificación de texto y traducción automática. Por ejemplo, en el análisis de sentimiento, el modelo tomaría un texto y lo convertiría en un vector, que luego se puede alimentar a un clasificador para predecir si el sentimiento del texto es positivo o negativo.\n",
    "\n",
    "### Agentes\n",
    "\n",
    "Los agentes usan LLM para decidir qué acciones se deben tomar, se pueden usar herramientas como búsqueda web o calculadoras, y todo se empaqueta en un ciclo lógico de operaciones.\n",
    "\n",
    "\n",
    "### Memoria\n",
    "\n",
    "Por defecto, las cadenas y los agentes no tienen estado, lo que significa que tratan cada consulta de forma independiente, en algunas aplicaciones (los chatbots son un GRAN ejemplo) es muy importante recordar las interacciones anteriores, tanto a corto como a largo plazo, el concepto de Memoria en LangChain existe precisamente para hacer eso.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Construyendo una Aplicacion de Modelo de Lenguaje: LLMs\n",
    "\n",
    "Ahora que hemos instalado LangChain y configurado nuestro entorno, podemos comenzar a construir nuestra aplicación de modelo de lenguaje.\n",
    "\n",
    "### LLMs: Obtener predicciones de un modelo de lenguaje\n",
    "\n",
    "El bloque de construcción más básico de LangChain es llamar a un LLM en alguna entrada.\n",
    "\n",
    "Para este ejemplo, pretendamos que estamos construyendo un servicio que genera nombres para ideas de negocio basado en lo que hace el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087421bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calcetines Rainbow Joy.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Inicializamos el LLM\n",
    "# En este ejemplo, probablemente queramos que las salidas sean MÁS aleatorias.\n",
    "# para esto utilizamos el argumento temperature.\n",
    "# nos permite controlar que tan deterministicas son las respuestas.\n",
    "# donde 0 significa que son deterministicas y 1 totalmente aleatorias.\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# Indicamos nuestra entrada\n",
    "text = \"Cual seria un buen nombre para una empresa que vende calcetines coloridos?\"\n",
    "\n",
    "# Llamamos al modelo pasando como argumento nuestra entrada\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f17b59",
   "metadata": {},
   "source": [
    "### Prompt Templates: Administrar prompts para LLMs\n",
    "\n",
    "Normalmente cuando utilizamos un LLM en una aplicacion, no enviamos la entrada del usuario directamente an LLM, sino que queremos tomar la entrada del usuario, construir un prompt adecuado y enviar eso al LLM.\n",
    "\n",
    "En el ejemplo anterior, el texto de entrada estaba predeterminado para pedir el nombre de una empresa que fabricaba calcetines coloridos, lo que quisieramos hacer es solamente tomar la entrada del usuario sobre que hace la empresa y crear un prompt con esta informacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541ac59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cual seria un buen nombre para una empresa que vende panqueques esponjosos\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Instanciamos un PromptTemplate donde indicamos nuestra entrada predeterminada y las variables a cambiar\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Cual seria un buen nombre para una empresa que vende {product}\"\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"panqueques esponjosos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7807b5",
   "metadata": {},
   "source": [
    "## Cadenas: Combinando LLMs y prompts \n",
    "\n",
    "Una cadena en LangChain esta compuesta por multiples componentes, que pueden ser primitivos como LLMs o otras cadenas.\n",
    "\n",
    "El tipo de cadena mas basico es un LLMChain que consta de un PromptTemplate y un LLM.\n",
    "\n",
    "Extendiendo el ejemplo anterior, podemos construir un LLMChain que toma la entrada del usuario, lo formatea con un PromptTemplate y luego pasa la respuesta formateada a un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "798e3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Fluffy Pancake Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Ahora podemos correr nuestra cadena solamente especificando el producto deseado.\n",
    "print(chain.run(\"Panqueques esponjosos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bbe83",
   "metadata": {},
   "source": [
    "### Agentes: Llama cadenas dinamicamente acorde a la entrada del usuario\n",
    "\n",
    "Las cadenas por defecto corren en un orden predeterminado.\n",
    "\n",
    "En contrario, los agentes no, utilizan un LLM para determinar que acciones y en que orden.\n",
    "\n",
    "En orden para cargar agentes, debe entender los siguientes conceptos:\n",
    "\n",
    "- Herramientas: Funciones que ejecutan tareas especificas, pueden ser cosas como: Busquedas de Google, Busquedas en Bases de Datos, REPLs de Python, interactuar con otras cadenas. La interfaz de una herramienta actualmente es una función que se espera que tenga un string como entrada, con string como salida.\n",
    "- LLM: El modelo de lenguaje que utiliza el agente.\n",
    "- Agente: El agente a utilizar, especificado en formato string.\n",
    "\n",
    "Para este ejemplo, necesitaremos instalar el paquete de SerpAPI de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97913c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/blvck/.local/lib/python3.8/site-packages (from google-search-results) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->google-search-results) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->google-search-results) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/blvck/.local/lib/python3.8/site-packages (from requests->google-search-results) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->google-search-results) (1.25.8)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=df7da4487fe72432a93cf4fa9396a6cf6b36d9c7f54ae412ff01618e01fe8950\n",
      "  Stored in directory: /home/blvck/.cache/pip/wheels/29/75/71/9bf68178a74593837f73b6e9d9a070d45d308bddfd2e95290a\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef6b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"340875be05ae86bd95fbf3b103824deb90eea48d75ea5b1532b721ee28d01666\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e804fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the current temperature in San Francisco\n",
      "Action: Search\n",
      "Action Input: \"current temperature in San Francisco\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTemperature History ; High. Low ; Forecast. 57°. 48° ; Average. 63°. 49° ; Last Year. 86°. 58° ; Record. 92°. 1989. 40°. 1891 ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert the temperature from Fahrenheit to Celsius\n",
      "Action: Calculator\n",
      "Action Input: 57°F to Celsius\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 13.88888888888889\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: La temperatura actual en San Francisco es de 13.88 grados Celsius.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La temperatura actual en San Francisco es de 13.88 grados Celsius.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "# Primero, cargamos el modelo de lenguaje a utilizar.\n",
    "# En este caso, indicamos una temperatura 0.\n",
    "# Debido a que queremos que las respuestas sean deterministicas y no aleatorias.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# tiene varias opciones de modelos pero \n",
    "# Posteriormente cargamos las herramientas a utilizar y el llm.\n",
    "# 1. serpapi para las busquedas.\n",
    "# 2. llm-math para operaciones matematicas\n",
    "# llm-math utiliza un llm y por eso pasamos el llm como segundo parametro.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# FInalmente, inicializamos un agente con las herramientas, el llm y el tipo de agente a utilizar.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"Cual es la temperatura actual en San Francisco en grados Celsios?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b5a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who won the 2023 World Cup.\n",
      "Action: Search\n",
      "Action Input: 2023 World Cup winner\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFIFA Club World Cup 2022 in 2023: All winners - complete list ... Real Madrid claimed their fifth FIFA Club World Cup title in Morocco, putting ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to narrow down the search results to the World Cup\n",
      "Action: Search\n",
      "Action Input: 2023 World Cup winner national team\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUEFA Champions League holders Real Madrid won a record fifth Club World Cup in Morocco, beating Saudi champs Al-Hilal 5-3 in the final.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Real Madrid\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Real Madrid'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Quien salio campeon en el mundial de selecciones de futbol 2023?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
